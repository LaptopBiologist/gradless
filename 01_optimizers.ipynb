{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizers\n",
    "\n",
    "The general class used to perform gradient descent is ```optimizers.GradientDescent``` which is modelled after the default implementation of Spall's SPSA optimization scheme outlined [here](https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF). However, this can be modified by choosing different update rules to embed the SPSA gradient estimate inside more efficient gradient descent algorithms, such as ADAM and ADAGRAD.\n",
    "\n",
    "The ```GradientDescent``` class has two general classes of parameters:\n",
    "\n",
    "1. Functions that determine how parameters are updated (cost, update, gradient)\n",
    "2. Parameters related to how steps are performed and the gradient is approximated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pymc3 as pm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy\n",
    "import scipy\n",
    "from gradless.gradient import SPSAGradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SPSAGradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d467f81ba397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def __init__(self,x_0, cost, update, gradient=SPSAGradient(), acceptance_rule=None,\n\u001b[1;32m      4\u001b[0m                  \u001b[0mparam_stepsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_stepdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_decay_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mgrad_stepsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_stepdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d467f81ba397>\u001b[0m in \u001b[0;36mGradientDescent\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     def __init__(self,x_0, cost, update, gradient=SPSAGradient(), acceptance_rule=None,\n\u001b[0m\u001b[1;32m      4\u001b[0m                  \u001b[0mparam_stepsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_stepdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_decay_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mgrad_stepsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_stepdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SPSAGradient' is not defined"
     ]
    }
   ],
   "source": [
    "#export\n",
    "class GradientDescent():\n",
    "    def __init__(self,x_0, cost, update, gradient=SPSAGradient(), acceptance_rule=None,\n",
    "                 param_stepsize=1, param_stepdecay=.4, param_decay_offset=0, \n",
    "                 grad_stepsize=1, grad_stepdecay=.2, \n",
    "                seed=None):\n",
    "        if seed is not None:\n",
    "            assert type(seed) is int\n",
    "            numpy.random.seed(seed)\n",
    "        self.cost=cost\n",
    "        self.update=update\n",
    "        self.gradient=gradient\n",
    "        \n",
    "        #if the gradient was passed without cost being defined, set the cost\n",
    "        if self.gradient.cost is None:\n",
    "            self.gradient.set_cost(self.cost)\n",
    "            \n",
    "        self.param_stepsize=param_stepsize\n",
    "        self.param_stepdecay=param_stepdecay\n",
    "        self.param_decay_offset=param_decay_offset\n",
    "        self.grad_stepsize=grad_stepsize\n",
    "        self.grad_stepdecay=grad_stepdecay\n",
    "        self.t=0.\n",
    "        self.cost_history=[cost.evaluate(x_0)]\n",
    "\n",
    "        self.theta_hist=[x_0]\n",
    "        self.theta=x_0\n",
    "        \n",
    "        self.acceptance_rule=acceptance_rule\n",
    "        if self.acceptance_rule is not None:\n",
    "            self.acceptance_rule.initialize(self)\n",
    "\n",
    "    def update_params (self, gradient_reps=1,block_val=None, update_rvs=False):\n",
    "        \"\"\"This performs a single update of the model parameters\"\"\"\n",
    "        self.t+=1\n",
    "        \n",
    "        c_k=self.grad_step()\n",
    "        ### get the gradient\n",
    "        ghat= self.gradient.evaluate( self.theta, c_k, gradient_reps=gradient_reps, update_rvs=update_rvs )\n",
    "\n",
    "        \n",
    "        ### determine the proposed step\n",
    "        a_k=self.param_step()\n",
    "        step=self.update.evaluate(ghat, a_k ,self.t)\n",
    "\n",
    "\n",
    "\n",
    "        ### update the parameters\n",
    "        new_theta=self.theta-step\n",
    "        new_cost=self.cost.evaluate(new_theta)\n",
    "        \n",
    "        #I want to replace this with an acceptance rule\n",
    "        \n",
    "        #Always reject nans\n",
    "        if numpy.isnan(new_cost):\n",
    "            self.t-=1\n",
    "            return()\n",
    "        \n",
    "        #Evaluate the acceptance criterion here\n",
    "        if self.acceptance_rule is not None:\n",
    "            accept=self.acceptance_rule.evaluate(new_cost, self.t)\n",
    "            if accept==False:\n",
    "                self.t-=1\n",
    "                return() \n",
    "                \n",
    "        if block_val is not None:\n",
    "            if self.t<100:\n",
    "                if new_cost>(1.5*self.cost_history[-1]):\n",
    "                    self.t-=1\n",
    "                    return() \n",
    "            else:\n",
    "#                 mean_cost=numpy.mean(self.cost_history[-100:])\n",
    "                sd_cost=numpy.std(self.cost_history[-100:])\n",
    "                if new_cost>(block_val*sd_cost+self.cost_history[-1]):\n",
    "                    self.t-=1\n",
    "                    return()\n",
    "\n",
    "        ### evaluate the objective function\n",
    "        \n",
    "        self.theta_hist.append(new_theta)\n",
    "        self.theta=new_theta\n",
    "        \n",
    "        self.cost_history.append(new_cost)\n",
    "        \n",
    "    def fit(self, niter=10000, init_grad_reps=100):\n",
    "        \"\"\"This performs a set number of gradient descent descent iterations, along with some initialization\"\"\"\n",
    "        pass\n",
    "    def param_step(self):\n",
    "        \"\"\"This determines the step size used to update the model parameters.\n",
    "        \n",
    "        a_k= a/(t+A)**alpha\"\"\"\n",
    "        return  (self.param_stepsize / (self.t+self.param_decay_offset)**self.param_stepdecay)\n",
    "        \n",
    "    def grad_step(self):\n",
    "        \"\"\"This determines the step size used to perturb the parameters during the gradient approximation\"\"\"\n",
    "        return (self.grad_stepsize/(self.t)**self.grad_stepdecay)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization\n",
    "\n",
    "There are two main sets of parameters that need to be chosen: parameters determining the size of the update step and parameters determining the size the of the step used to estimate the gradient.\n",
    "\n",
    "It would be useful to have some functions for choosing these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_param_step(opt,gradient_reps=100):\n",
    "    opt.t+=1.\n",
    "    c_k=opt.grad_step()\n",
    "    ### get the gradient\n",
    "    ghat= opt.gradient.evaluate(opt.cost, opt.theta, c_k, gradient_reps=gradient_reps, update_rvs=True )\n",
    "\n",
    "    ### determine the proposed step\n",
    "    a_k=opt.param_step()\n",
    "    step=opt.update.evaluate(ghat, a_k ,opt.t)\n",
    "    current_cost=opt.cost.evaluate(theta)\n",
    "    for k in range(20):\n",
    "        test_theta=opt.theta-step/2.**k\n",
    "        test_cost=opt.cost.evaluate(test_theta)\n",
    "        \n",
    "    return a_k/2.**k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting initial parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
