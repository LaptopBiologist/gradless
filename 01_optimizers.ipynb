{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizers\n",
    "\n",
    "The general class used to perform gradient descent is ```optimizers.GradientDescent()``` which is modelled after the default implementation of Spall's SPSA optimization scheme outlined in [placeholder](placeholder). However, this can be modified by choosing different update rules to embed the SPSA gradient estimate inside more efficient gradient descent algorithms, such as ADAM and ADAGRAD.\n",
    "\n",
    "The ```GradientDescent``` class has two general classes of parameters:\n",
    "\n",
    "1. Functions that determine how parameters are updated (cost, update, gradient)\n",
    "2. Parameters related to how steps are performed and the gradient is approximated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import numpy\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent():\n",
    "    def __init__(self,cost, update, gradient, \n",
    "                 param_stepsize=1, param_stepdecay=.4, param_decay_offset=0, \n",
    "                 grad_stepsize=1, grad_stepdecay=.2, ):\n",
    "        self.cost=cost\n",
    "        self.update=update\n",
    "        self.gradient=gradient\n",
    "        self.param_stepsize=param_stepsize\n",
    "        self.param_stepdecay=param_stepdecay\n",
    "        self.param_decay_offset=param_decay_offset\n",
    "        self.grad_stepsize=grad_stepsize\n",
    "        self.grad_stepdecay=grad_stepdecay\n",
    "        self.t=0\n",
    "        self.cost_history=[]\n",
    "        self.param_step\n",
    "    def update_params (self):\n",
    "        self.t+=1\n",
    "        ### do an update of the parameters\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
