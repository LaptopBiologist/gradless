{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizers - The GradientDescent implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class used to perform gradient descent.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pymc3 as pm\n",
    "import hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy\n",
    "import scipy\n",
    "from gradless.gradient import SPSAGradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientDescent():\n",
    "    def __init__(self,x_0, model, update, gradient=None, acceptance_rule=None,\n",
    "                 param_stepsize=1, param_stepdecay=.4, param_decay_offset=0, \n",
    "                 grad_stepsize=1, grad_stepdecay=.2, \n",
    "                seed=None):\n",
    "        if seed is not None:\n",
    "            assert type(seed) is int\n",
    "            numpy.random.seed(seed)\n",
    "        # store the model\n",
    "        self.cost=model\n",
    "        \n",
    "        # Call the model once to ensure evaluate returns a float\n",
    "        \n",
    "#         test_val=self.cost.evaluate(x_0)\n",
    "#         print (isinstance(test_val,float))\n",
    "        try: \n",
    "            numpy.isnan(self.cost.evaluate(x_0))\n",
    "        except: raise(AssertionError(\"The cost function must return a float or an array with shape (1,) (e.g. not an array)\"))\n",
    "#         assert isinstance(test_val,float) or test_val.shape==(1), \"The cost function must return a float or an array with shape (1,) (e.g. not an array)\"\n",
    "        \n",
    "        self.update=update\n",
    "        if gradient is None: gradient=SPSAGradient()\n",
    "        self.gradient=gradient\n",
    "        \n",
    "        #if the gradient was passed without cost being defined, set the cost\n",
    "        if self.gradient.cost is None:\n",
    "            self.gradient.set_cost(self.cost)\n",
    "            \n",
    "        self.param_stepsize=param_stepsize\n",
    "        self.param_stepdecay=param_stepdecay\n",
    "        self.param_decay_offset=param_decay_offset\n",
    "        self.grad_stepsize=grad_stepsize\n",
    "        self.grad_stepdecay=grad_stepdecay\n",
    "        self.t=0.\n",
    "        self.cost_history=[self.cost.evaluate(x_0)]\n",
    "\n",
    "        self.theta_hist=[x_0]\n",
    "        self.theta=x_0\n",
    "        \n",
    "        self.acceptance_rule=acceptance_rule\n",
    "        if self.acceptance_rule is not None:\n",
    "            self.acceptance_rule.initialize(self)\n",
    "\n",
    "    def update_params (self, gradient_reps=1,block_val=None, update_rvs=False):\n",
    "        \"\"\"This performs a single update of the model parameters\"\"\"\n",
    "        self.t+=1\n",
    "        \n",
    "        c_k=self.grad_step()\n",
    "        ### get the gradient\n",
    "        ghat= self.gradient.evaluate( self.theta, c_k, gradient_reps=gradient_reps, update_rvs=update_rvs )\n",
    "\n",
    "        \n",
    "        ### determine the proposed step\n",
    "        a_k=self.param_step()\n",
    "        step=self.update.evaluate(ghat, a_k ,self.t)\n",
    "\n",
    "\n",
    "\n",
    "        ### update the parameters\n",
    "        new_theta=self.theta-step\n",
    "        new_cost=self.cost.evaluate(new_theta)\n",
    "        \n",
    "        #I want to replace this with an acceptance rule\n",
    "        \n",
    "        #Always reject nans\n",
    "        if numpy.isnan(new_cost):\n",
    "            self.t-=1\n",
    "            return()\n",
    "        \n",
    "        #Evaluate the acceptance criterion here\n",
    "        if self.acceptance_rule is not None:\n",
    "            accept=self.acceptance_rule.evaluate(new_cost, self.t)\n",
    "            if accept==False:\n",
    "                self.t-=1\n",
    "                return() \n",
    "                \n",
    "        if block_val is not None:\n",
    "            if self.t<100:\n",
    "                if new_cost>(1.5*self.cost_history[-1]):\n",
    "                    self.t-=1\n",
    "                    return() \n",
    "            else:\n",
    "#                 mean_cost=numpy.mean(self.cost_history[-100:])\n",
    "                sd_cost=numpy.std(self.cost_history[-100:])\n",
    "                if new_cost>(block_val*sd_cost+self.cost_history[-1]):\n",
    "                    self.t-=1\n",
    "                    return()\n",
    "\n",
    "        ### evaluate the objective function\n",
    "        \n",
    "        self.theta_hist.append(new_theta)\n",
    "        self.theta=new_theta\n",
    "        \n",
    "        self.cost_history.append(new_cost)\n",
    "        \n",
    "    def fit(self, niter=10000, init_grad_reps=100):\n",
    "        \"\"\"This performs a set number of gradient descent descent iterations, along with some initialization\"\"\"\n",
    "        pass\n",
    "    def param_step(self):\n",
    "        \"\"\"This determines the step size used to update the model parameters.\n",
    "        \n",
    "        a_k= a/(t+A)**alpha\"\"\"\n",
    "        return  (self.param_stepsize / (self.t+self.param_decay_offset)**self.param_stepdecay)\n",
    "        \n",
    "    def grad_step(self):\n",
    "        \"\"\"This determines the step size used to perturb the parameters during the gradient approximation\"\"\"\n",
    "        return (self.grad_stepsize/(self.t)**self.grad_stepdecay)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general class used to perform gradient descent is `GradientDescent` which is modelled after the default implementation of Spall's SPSA optimization scheme outlined [here](https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF). However, this can be modified by choosing different update rules to embed the SPSA gradient estimate inside more efficient gradient descent algorithms, such as ADAM and ADAGRAD.\n",
    "\n",
    "### Usage\n",
    "\n",
    "\n",
    "The ```GradientDescent``` class has two general classes of arguments:\n",
    "\n",
    "1. Arguments that determine how model parameters are updated (cost, update, gradient)\n",
    "\n",
    " * x_0 (required): An initial guess of the model parameters where the optimizer will begin\n",
    " \n",
    " * model (required): The model to be optimized. Generally this should be an instance of the `Model` class.\n",
    " \n",
    " * update (required): This should be a class that proposes a parameter update based on the gradient. See `updates` for details\n",
    " \n",
    " * gradient : This should be an instance of a class that provides an estimate of the gradient. By default, uses `SPSAGradient`. See ```gradient``` for more details\n",
    " \n",
    " * acceptance_rule (optional): An AcceptanceRule class may be passed here to define rules for accepting or rejecting parameter updates. See `updates` for details\n",
    "\n",
    "2. Arguments related to how steps are performed (i.e. learning rate) and the gradient is approximated\n",
    "\n",
    "The following parameters are required and relate to how the parameters are updated\n",
    "\n",
    " * param_stepsize\n",
    " * param_stepdecay\n",
    " * param_decay_offset\n",
    "\n",
    "The learning rate at iteration ```t``` is calculated as\n",
    "\n",
    "```learning_rate = param_stepsize / (param_decay_offset + t) ** param_stepdecay```\n",
    "\n",
    "A constant\n",
    "\n",
    "The following parameters are required and relate to how the parameters are perturbed during the gradient approximation:\n",
    "\n",
    " * grad_stepsize\n",
    " * grad_stepdecay\n",
    "\n",
    "\n",
    "The perturbation step at iteration ```t``` is calculated as\n",
    "\n",
    "```C_t = grad_stepsize / ( t ** grad_stepdecay )```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hide\n",
    "\n",
    "## Tests\n",
    "\n",
    "I want to make sure I can initialize the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "import numpy\n",
    "from gradless import optimizers, costs, gradient, updates\n",
    "from matplotlib import pyplot\n",
    "import hypothesis\n",
    "from hypothesis import given\n",
    "import hypothesis.strategies as st\n",
    "import hypothesis.extra.numpy as hypo_numpy\n",
    "from hypothesis import note\n",
    "from numpy.testing import *\n",
    "\n",
    "\n",
    "def test_GradientDescent_initialization():\n",
    "    \"\"\"Tests to ensure GradientDescent objects are independent\n",
    "    by initializing multiple GradientDescent objects with different numbers of parameters \"\"\"\n",
    "    def quadratic(x):\n",
    "        return ((x)**2).sum()\n",
    " \n",
    "    true_value=0\n",
    "    for param_num in range(1,10):\n",
    "        start_value=numpy.array([50]*param_num)\n",
    "        model=costs.Model(quadratic)\n",
    "        update_rule=updates.StandardSPSA(max_step=.2)\n",
    "        opt=GradientDescent(start_value, model,update_rule,\n",
    "                                   param_stepsize = 2, param_stepdecay = .4, param_decay_offset = 0, \n",
    "                                   grad_stepsize = 1, grad_stepdecay = .2, seed=2 )\n",
    "        #perform two updates to make sure all function calls work\n",
    "        opt.update_params()\n",
    "        opt.update_params()\n",
    "test_GradientDescent_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make sure the optimizer still converges after whatever updates I've made. I'll find the minimum of a quadratic function $f(x)=\\sum_i x_i^2$ for both one and two parameters models. I'll run it for 1000 iterations each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "\n",
    "# I need a test to ensure the optimizer runs and hasn't been broken\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_GradientDescent_convergence_one_param():\n",
    "    \"\"\"Tests whether the optimizer converges on a simple 1-d problem\"\"\"\n",
    "    def quadratic(x):\n",
    "        return ((x)**2).sum()\n",
    "    start_value=numpy.array([50])\n",
    "    true_value=0\n",
    "    \n",
    "    model=costs.Model(quadratic)\n",
    "    update_rule=updates.StandardSPSA(max_step=.2)\n",
    "    opt=GradientDescent(start_value, model,update_rule,\n",
    "                               param_stepsize = 2, param_stepdecay = .4, param_decay_offset = 0, \n",
    "                               grad_stepsize = 1, grad_stepdecay = .2, seed=2 )\n",
    "\n",
    "    for i in range(1000):\n",
    "        opt.update_params()\n",
    "    \n",
    "    assert_almost_equal(opt.theta, true_value)\n",
    "\n",
    "test_GradientDescent_convergence_one_param()\n",
    "\n",
    "def test_GradientDescent_convergence_two_param():\n",
    "    \"\"\"Tests whether the optimizer converges on a simple 2-d problem\"\"\"\n",
    "    def quadratic(x):\n",
    "        return ((x)**2).sum()\n",
    "    start_value=numpy.array([50,-25])\n",
    "    true_value=numpy.array([0,0])\n",
    "    \n",
    "    model=costs.Model(quadratic)\n",
    "    update_rule=updates.StandardSPSA(max_step=.2)\n",
    "    opt=GradientDescent(start_value, model,update_rule,\n",
    "                               param_stepsize = 2, param_stepdecay = .4, param_decay_offset = 0, \n",
    "                               grad_stepsize = 1, grad_stepdecay = .2, seed=2 )\n",
    "\n",
    "    for i in range(1000):\n",
    "        opt.update_params()\n",
    "    \n",
    "    assert_almost_equal(opt.theta, true_value)\n",
    "\n",
    "test_GradientDescent_convergence_two_param()\n",
    "    \n",
    "# model=costs.Model(quadratic)\n",
    "# update_rule=updates.StandardSPSA(max_step=.2)\n",
    "# test_convergence(numpy.array([50.,-25]),[0,0], model,update_rule)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
