{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizers\n",
    "\n",
    "The general class used to perform gradient descent is ```optimizers.GradientDescent()``` which is modelled after the default implementation of Spall's SPSA optimization scheme outlined in [placeholder](placeholder). However, this can be modified by choosing different update rules to embed the SPSA gradient estimate inside more efficient gradient descent algorithms, such as ADAM and ADAGRAD.\n",
    "\n",
    "The ```GradientDescent``` class has two general classes of parameters:\n",
    "\n",
    "1. Functions that determine how parameters are updated (cost, update, gradient)\n",
    "2. Parameters related to how steps are performed and the gradient is approximated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.gradient'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e945e582e0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSPSAGradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.gradient'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import numpy\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientDescent():\n",
    "    def __init__(self,x_0, cost, update, gradient, \n",
    "                 param_stepsize=1, param_stepdecay=.4, param_decay_offset=0, \n",
    "                 grad_stepsize=1, grad_stepdecay=.2, ):\n",
    "        self.cost=cost\n",
    "        self.update=update\n",
    "        self.gradient=gradient\n",
    "        self.param_stepsize=param_stepsize\n",
    "        self.param_stepdecay=param_stepdecay\n",
    "        self.param_decay_offset=param_decay_offset\n",
    "        self.grad_stepsize=grad_stepsize\n",
    "        self.grad_stepdecay=grad_stepdecay\n",
    "        self.t=0\n",
    "        self.cost_history=[]\n",
    "\n",
    "        self.theta_hist=[x_0]\n",
    "        self.theta=x_0\n",
    "\n",
    "    def update_params (self, gradient_reps=1):\n",
    "        \"\"\"This performs a single update of the model parameters\"\"\"\n",
    "        self.t+=1\n",
    "        \n",
    "        c_k=self.grad_step()\n",
    "        ### get the gradient\n",
    "        ghat= self.gradient.evaluate(self.cost, self.theta, c_k, gradient_reps )\n",
    "        \n",
    "        ### determine the proposed step\n",
    "        a_k=self.param_step()\n",
    "        step=self.update.evaluate(ghat, a_k ,t)\n",
    "        \n",
    "\n",
    "        ### update the parameters\n",
    "        new_theta=theta-step\n",
    "        new_cost=self.cost.evaluate(new_theta)\n",
    "        \n",
    "        ### evaluate the objective function\n",
    "        \n",
    "        self.theta_hist.append(new_theta)\n",
    "        self.theta=new_theta\n",
    "        \n",
    "        self.cost_history.append(new_cost)\n",
    "        \n",
    "    def fit(self, niter=10000, init_grad_reps=100):\n",
    "        \"\"\"This performs a set number of gradient descent descent iterations, along with some initialization\"\"\"\n",
    "        pass\n",
    "    def param_step(self):\n",
    "        \"\"\"This determines the step size used to update the model parameters.\n",
    "        \n",
    "        a_k= a/(t+A)**alpha\"\"\"\n",
    "        return  (self.param_stepsize / (self.t+self.param_decay_offset)**self.param_stepdecay)\n",
    "        \n",
    "    def grad_step(self):\n",
    "        \"\"\"This determines the step size used to perturb the parameters during the gradient approximation\"\"\"\n",
    "        return (self.grad_stepsize/(self.t)**self.grad_stepdecay)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
