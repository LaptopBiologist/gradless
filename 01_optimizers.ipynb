{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizers\n",
    "\n",
    "The general class used to perform gradient descent is ```optimizers.GradientDescent()``` which is modelled after the default implementation of Spall's SPSA optimization scheme outlined in [placeholder](placeholder). However, this can be modified by choosing different update rules to embed the SPSA gradient estimate inside more efficient gradient descent algorithms, such as ADAM and ADAGRAD.\n",
    "\n",
    "The ```GradientDescent``` class has two general classes of parameters:\n",
    "\n",
    "1. Functions that determine how parameters are updated (cost, update, gradient)\n",
    "2. Parameters related to how steps are performed and the gradient is approximated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import numpy\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent():\n",
    "    def __init__(self,x_0, cost, update, gradient, \n",
    "                 param_stepsize=1, param_stepdecay=.4, param_decay_offset=0, \n",
    "                 grad_stepsize=1, grad_stepdecay=.2, ):\n",
    "        self.cost=cost\n",
    "        self.update=update\n",
    "        self.gradient=gradient\n",
    "        self.param_stepsize=param_stepsize\n",
    "        self.param_stepdecay=param_stepdecay\n",
    "        self.param_decay_offset=param_decay_offset\n",
    "        self.grad_stepsize=grad_stepsize\n",
    "        self.grad_stepdecay=grad_stepdecay\n",
    "        self.t=0\n",
    "        self.cost_history=[]\n",
    "\n",
    "        self.theta_hist=[x_0]\n",
    "        self.theta=x_0\n",
    "\n",
    "    def update_params (self, gradient_reps=1):\n",
    "        \"\"\"This performs a single update of the model parameters\"\"\"\n",
    "        self.t+=1\n",
    "        \n",
    "        c_k=self.grad_step()\n",
    "        ### get the gradient\n",
    "        ghat= self.gradient(self.cost, self.theta, c_k, gradient_reps )\n",
    "        \n",
    "        ### determine the proposed step\n",
    "        a_k=self.param_step()\n",
    "        step=self.update(self.theta, a_k ,t)\n",
    "        \n",
    "\n",
    "        ### update the parameters\n",
    "        new_theta=theta-step\n",
    "        new_cost=self.cost(new_theta)\n",
    "        \n",
    "        ### evaluate the objective function\n",
    "        \n",
    "        self.theta_hist.append(new_theta)\n",
    "        self.theta=new_theta\n",
    "        \n",
    "        self.cost_history.append(new_cost)\n",
    "        \n",
    "    def fit(self, niter=10000, init_grad_reps=100):\n",
    "        \"\"\"This performs a set number of gradient descent descent iterations, along with some initialization\"\"\"\n",
    "        pass\n",
    "    def param_step(self):\n",
    "        \"\"\"This determines the step size used to update the model parameters.\n",
    "        \n",
    "        a_k= a/(t+A)**alpha\"\"\"\n",
    "        return  (self.param_stepsize / (self.t+self.param_decay_offset)**self.param_stepdecay)\n",
    "        \n",
    "    def grad_step(self):\n",
    "        \"\"\"This determines the step size used to perturb the parameters during the gradient approximation\"\"\"\n",
    "        return (self.grad_stepsize/(self.t)**self.grad_stepdecay)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
