---

title: optimizers - The GradientDescent implementation


keywords: fastai
sidebar: home_sidebar



nb_path: "01_optimizers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_optimizers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The class used to perform gradient descent.</p>
<hr>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GradientDescent" class="doc_header"><code>class</code> <code>GradientDescent</code><a href="https://github.com/LaptopBiologist/gradless/tree/master/gradless/optimizers.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GradientDescent</code>(<strong><code>x_0</code></strong>, <strong><code>model</code></strong>, <strong><code>update</code></strong>, <strong><code>gradient</code></strong>=<em><code>None</code></em>, <strong><code>acceptance_rule</code></strong>=<em><code>None</code></em>, <strong><code>param_stepsize</code></strong>=<em><code>1</code></em>, <strong><code>param_stepdecay</code></strong>=<em><code>0.4</code></em>, <strong><code>param_decay_offset</code></strong>=<em><code>0</code></em>, <strong><code>grad_stepsize</code></strong>=<em><code>1</code></em>, <strong><code>grad_stepdecay</code></strong>=<em><code>0.2</code></em>, <strong><code>seed</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The general class used to perform gradient descent is <a href="/gradless/optimizers.html#GradientDescent"><code>GradientDescent</code></a> which is modelled after the default implementation of Spall's SPSA optimization scheme outlined <a href="https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF">here</a>. However, this can be modified by choosing different update rules to embed the SPSA gradient estimate inside more efficient gradient descent algorithms, such as ADAM and ADAGRAD.</p>
<h3 id="Usage">Usage<a class="anchor-link" href="#Usage"> </a></h3><p>The <code>[`GradientDescent`](/gradless/optimizers.html#GradientDescent)</code> class has two general classes of arguments:</p>
<ol>
<li><p>Arguments that determine how model parameters are updated (cost, update, gradient)</p>
<ul>
<li><p>x_0 (required): An initial guess of the model parameters where the optimizer will begin</p>
</li>
<li><p>model (required): The model to be optimized. Generally this should be an instance of the <a href="/gradless/models.html#Model"><code>Model</code></a> class.</p>
</li>
<li><p>update (required): This should be a class that proposes a parameter update based on the gradient. See <a href="/gradless/updates.html"><code>updates</code></a> for details</p>
</li>
<li><p>gradient : This should be an instance of a class that provides an estimate of the gradient. By default, uses <a href="/gradless/gradient.html#SPSAGradient"><code>SPSAGradient</code></a>. See <code>[`gradient`](/gradless/gradient.html)</code> for more details</p>
</li>
<li><p>acceptance_rule (optional): An AcceptanceRule class may be passed here to define rules for accepting or rejecting parameter updates. See <a href="/gradless/updates.html"><code>updates</code></a> for details</p>
</li>
</ul>
</li>
<li><p>Arguments related to how steps are performed (i.e. learning rate) and the gradient is approximated</p>
</li>
</ol>
<p>The following parameters are required and relate to how the parameters are updated</p>
<ul>
<li>param_stepsize</li>
<li>param_stepdecay</li>
<li>param_decay_offset</li>
</ul>
<p>The learning rate at iteration <code>t</code> is calculated as</p>
<p><code>learning_rate = param_stepsize / (param_decay_offset + t) ** param_stepdecay</code></p>
<p>A constant</p>
<p>The following parameters are required and relate to how the parameters are perturbed during the gradient approximation:</p>
<ul>
<li>grad_stepsize</li>
<li>grad_stepdecay</li>
</ul>
<p>The perturbation step at iteration <code>t</code> is calculated as</p>
<p><code>C_t = grad_stepsize / ( t ** grad_stepdecay )</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">gradless</span> <span class="kn">import</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">costs</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">updates</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">import</span> <span class="nn">hypothesis</span>
<span class="kn">from</span> <span class="nn">hypothesis</span> <span class="kn">import</span> <span class="n">given</span>
<span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">hypothesis.extra.numpy</span> <span class="k">as</span> <span class="nn">hypo_numpy</span>
<span class="kn">from</span> <span class="nn">hypothesis</span> <span class="kn">import</span> <span class="n">note</span>
<span class="kn">from</span> <span class="nn">numpy.testing</span> <span class="kn">import</span> <span class="o">*</span>


<span class="k">def</span> <span class="nf">test_GradientDescent_initialization</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Tests to ensure GradientDescent objects are independent</span>
<span class="sd">    by initializing multiple GradientDescent objects with different numbers of parameters &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">quadratic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
 
    <span class="n">true_value</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">param_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">start_value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">50</span><span class="p">]</span><span class="o">*</span><span class="n">param_num</span><span class="p">)</span>
        <span class="n">model</span><span class="o">=</span><span class="n">costs</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">quadratic</span><span class="p">)</span>
        <span class="n">update_rule</span><span class="o">=</span><span class="n">updates</span><span class="o">.</span><span class="n">StandardSPSA</span><span class="p">(</span><span class="n">max_step</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">opt</span><span class="o">=</span><span class="n">GradientDescent</span><span class="p">(</span><span class="n">start_value</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span><span class="n">update_rule</span><span class="p">,</span>
                                   <span class="n">param_stepsize</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">param_stepdecay</span> <span class="o">=</span> <span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="n">param_decay_offset</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                                   <span class="n">grad_stepsize</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grad_stepdecay</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2</span> <span class="p">)</span>
        <span class="c1">#perform two updates to make sure all function calls work</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">update_params</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">update_params</span><span class="p">()</span>
<span class="n">test_GradientDescent_initialization</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I want to make sure the optimizer still converges after whatever updates I've made. I'll find the minimum of a quadratic function $f(x)=\sum_i x_i^2$ for both one and two parameters models. I'll run it for 1000 iterations each time.</p>

</div>
</div>
</div>
</div>
 

