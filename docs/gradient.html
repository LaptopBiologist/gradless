---

title: gradient - Approximating the gradient


keywords: fastai
sidebar: home_sidebar



nb_path: "04_gradient.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 04_gradient.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A collection of classes and functions used to approximate the gradient.</p>
<hr>
<h2 id="Background">Background<a class="anchor-link" href="#Background"> </a></h2><p>Spall's simultaneous perturbation stochastic approximation (SPSA) of the gradient provides an efficient means to approximate the gradient of high-dimensional models, even when only noisy evaluations of the objective function are available. This is in constrast to more typical applications of stochastic gradient descent, where the noisiness of the gradient comes not from the objective function itself, but rather from evaluating the gradient on subsets of the data.</p>
<h3 id="Approximating-the-gradient-with-SPSA">Approximating the gradient with SPSA<a class="anchor-link" href="#Approximating-the-gradient-with-SPSA"> </a></h3><p>The general idea of SPSA is reasonably straightforward. Given a step size $c_t$ and a vector of perturbations $\delta$, we first generate forward and backward perturbations all model parameters simultaneously</p>
<p>{% raw %}
$$\theta^+ = \theta + c_t \delta$$
{% endraw %}
{% raw %}
$$\theta^- = \theta - c_t \delta$$
{% endraw %}</p>
<p>The perturbation, $\delta$ is often sampled from a shifted and rescaled Bernoulli distribution as follows:</p>
<p>{% raw %}
$$b_1, b_2,..., b_m \sim Bernoulli(p=.5)$$
{% endraw %}
{% raw %}
$$\delta_i = 2b_i -1$$
{% endraw %}</p>
<p>where $\delta_i$ is the direction in which the $i$-th model parameter will be moved in the forward perturbation.</p>
<p>We then evaluate the cost function $F(\theta, X)$ at the two perturbed parameters</p>
<p>{% raw %}
$$y^+ = F(\theta^+, X)$$
{% endraw %}
{% raw %}
$$y^- = F(\theta^-, X)$$
{% endraw %}</p>
<p>The gradient is approximated as the slope of the line between the points $(\theta^+, y^+)$ and $(\theta^-, y^-)$:</p>
<p>{% raw %}
$$\hat{g}= \frac{y^+-y^-}{\theta^+ - \theta^-}= \frac{y^+-y^-}{2 c_t \delta}$$
{% endraw %}</p>
<p>A major advantage of this approximation is that in its simplest form, only two evaluations of the cost function are required, regardless of the dimensionality of the model. This is in constrast to the <a href="">finite-differences approximation</a> which requires each model parameter be perturbed separately.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GradientBase" class="doc_header"><code>class</code> <code>GradientBase</code><a href="https://github.com/LaptopBiologist/gradless/tree/master/gradless/gradient.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GradientBase</code>() :: <code>ABC</code></p>
</blockquote>
<p>A helper class that provides a standard means to create
classes to provide gradients or their approximations to GradientDescent.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><code>[`GradientDescent`](/gradless/optimizers.html#GradientDescent)</code> must be passed an object with a method called <code>.evaluate()</code>. This should store as an attribute the cost function to be evaluated and take the following inputs:</p>
<ol>
<li>theta - A 1-D numpy array of model parameters</li>
<li>c_k - A step size that may be used in the gradient evaluation</li>
<li>gradient_reps - The number of times to evaluate the gradient (multiple evaluations will be averaged)</li>
<li>update_rvs - Whether regenerated random variables stored in the cost function after each gradient evaluation</li>
</ol>
<p>It should return a vector of the same length as <code>theta</code> containing an estimate of the cost function's gradient at <code>theta</code>.</p>
<p>Any approach to gradient evaluation will require the first argument, <code>theta</code>. The latter three are only necessary when using an approximation of the gradient.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SPSAGradient" class="doc_header"><code>class</code> <code>SPSAGradient</code><a href="https://github.com/LaptopBiologist/gradless/tree/master/gradless/gradient.py#L21" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SPSAGradient</code>(<strong><code>param_subsets</code></strong>=<em><code>None</code></em>, <strong><code>fraction</code></strong>=<em><code>None</code></em>, <strong><code>cost</code></strong>=<em><code>None</code></em>) :: <a href="/gradless/gradient.html#GradientBase"><code>GradientBase</code></a></p>
</blockquote>
<p>A class for computing the SPSA gradient estimate.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>The <a href="/gradless/gradient.html#SPSAGradient"><code>SPSAGradient</code></a> class is used by <a href="/gradless/optimizers.html#GradientDescent"><code>GradientDescent</code></a> to approximate the gradient of an objective function, which can then be used to update model parameters.</p>
<p>This takes two arguments, both of which are optional:</p>
<ol>
<li><p><code>param_subsets</code> (optional) - A list or array of labels that defines groups of parameters. For example, [0,0,0,1,1,1] defines the first three model parameters as group 0 and the last three as belong to group 1.</p>
</li>
<li><p><code>cost</code> (optional) - The cost function used in the gradient evaluation. When passing an instance of the <a href="/gradless/gradient.html#SPSAGradient"><code>SPSAGradient</code></a> class to the <a href="/gradless/optimizers.html#GradientDescent"><code>GradientDescent</code></a> optimizer, this should be left undefined. The <a href="/gradless/optimizers.html#GradientDescent"><code>GradientDescent</code></a> object will automatically add the cost function being optimized to the <a href="/gradless/gradient.html#SPSAGradient"><code>SPSAGradient</code></a> if its cost function has not been defined.</p>
</li>
</ol>
<h4 id="Perturbing-subset-of-parameters">Perturbing subset of parameters<a class="anchor-link" href="#Perturbing-subset-of-parameters"> </a></h4><p>In some models, it might be desirable to evaluate the gradient separately for different subsets of parameters. For example, in variational inference, the means of the posterior approximation have a much stronger impact on the loss function than the standard deviations do. In that case, perturbing all parameters at once is likely to pick up the impact of perturbing the means on the gradient, but perhaps not the standard deviations.</p>
<p>The <code>param_labels</code> option permits to the gradient approximation to be evaluated separately for subsets of parameters. If, for example. <code>param_labels=[0,0,0,1,1,1]</code>, then the gradient will be approximated in two steps. The gradient will be estimated first for the three first parameters, perturbing them while holding the other parameters constant. Then the parameters labelled <code>1</code> will be perturbed, while all others are held constant. The cost of doing this is the number of cost function evaluations increases from $2$ to $2n$, where is $n$ number of parameter subset to be evaluated separately.</p>
<h4 id="Averaging-multiple-gradient-approximations">Averaging multiple gradient approximations<a class="anchor-link" href="#Averaging-multiple-gradient-approximations"> </a></h4><p>By default calling <code>evaluate</code> approximates the gradient from a single forward and backward perturbation. The argument <code>gradient_reps</code> can instead be set to an integer value greater than 1, to instead return the average of multiple gradient evaluations. If <code>gradient_reps</code> is set to $r$, <code>evaluate</code> will return the average of $r$ gradient approximations. This may lead to faster convergences.</p>

</div>
</div>
</div>
</div>
 

