{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updates\n",
    "\n",
    "The various update rules implemented in ```gradless``` are collected here. Each update rules is constructed as a class so that they can store any information they need to update the parameters, but all updates return a numpy array of floats to be used as a step to update the model parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy\n",
    "import scipy\n",
    "from abc import ABC, abstractmethod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class UpdateBase(ABC):\n",
    "    @abstractmethod\n",
    "    #This is the workhorse of the class\n",
    "    def evaluate(self): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step(theta, nu, m_t,v_t,G_t, beta1, beta2, t, epsilon ):\n",
    "    m_hat=m_t/(1-beta1**t)\n",
    "    v_hat=v_t/(1-beta2**t)\n",
    "    \n",
    "    theta_new=nu*m_hat/(v_hat**.5+epsilon)\n",
    "    return theta_new\n",
    "\n",
    "def get_step_adagrad(theta, nu, m_t,v_t,G_t, beta1, beta2, t, epsilon ):\n",
    "\n",
    "    theta_new=m_t*nu/(G_t+epsilon)**.5\n",
    "    return theta_new\n",
    "\n",
    "def get_step_NADAM(theta, nu, m_t,v_t,g_t, beta1, beta2, t, epsilon ):\n",
    "    if t>=2:\n",
    "        m_hat=m_t/(1-beta1**(t-1))\n",
    "    else:\n",
    "        m_hat=0.\n",
    "    v_hat=v_t/(1-beta2**t)\n",
    "\n",
    "    \n",
    "    part_1=(nu/(v_hat**.5+epsilon))\n",
    "    part_2=beta1*m_hat\n",
    "    part_3=(1-beta1)*g_t/(1-beta1**t)\n",
    "    theta_new=part_1*(part_2+part_3) \n",
    "    return theta_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard SPSA step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StandardSPSA(UpdateBase):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self,ghat, nu, t=0. ):\n",
    "\n",
    "        return nu*ghat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ADAGRAD step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ADAGRAD(UpdateBase):\n",
    "    def __init__(self):\n",
    "        self.G_t=None\n",
    "\n",
    "    def evaluate(self,ghat, nu, t=0. ):\n",
    "        if self.G_t is None:\n",
    "            self.G_t=numpy.zeros(ghat.shape)\n",
    "        self.G_t+=ghat**2\n",
    "        return nu*ghat/(self.G_t+self.eps)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ADAM step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ADAM(UpdateBase):\n",
    "    def __init__(self,beta1=.9, beta2=.999, eps=1e-8):\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.eps=eps\n",
    "        \n",
    "        self.m_t=[0.]\n",
    "        self.v_t=[0.]\n",
    "    def evaluate(self,ghat, nu, t ):\n",
    "        #Update the gradient histories\n",
    "        self.m_t.append(self.beta1*self.m_t[-1]+(1-self.beta1)*ghat)\n",
    "        self.v_t.append(self.beta2*self.v_t[-1]+(1-self.beta2)*ghat**2)\n",
    "        \n",
    "        #compute the bias corrections\n",
    "        m_hat=self.m_t[-1]/(1.-self.beta1**t)\n",
    "        v_hat=self.v_t[-1]/(1.-self.beta2**t)\n",
    "        \n",
    "        #compute the proposed step\n",
    "        return nu*m_hat/(v_hat**.5+self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nestorov-accelerated ADAM step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NADAM(UpdateBase):\n",
    "    def __init__(self,beta1=.9, beta2=.999, eps=1e-8):\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.eps=eps\n",
    "        \n",
    "        self.m_t=[0.]\n",
    "        self.v_t=[0.]\n",
    "    def evaluate(self,ghat, nu, t ):\n",
    "        #Update the gradient histories\n",
    "        self.m_t.append(self.beta1*self.m_t[-1]+(1-self.beta1)*ghat)\n",
    "        self.v_t.append(self.beta2*self.v_t[-1]+(1-self.beta2)*ghat**2)\n",
    "        \n",
    "\n",
    "        if t>=2:\n",
    "            m_hat=self.m_t[-1]/(1-self.beta1**(t-1))\n",
    "        else:\n",
    "            m_hat=0.\n",
    "        v_hat=self.v_t[-1]/(1-self.beta2**t)\n",
    "\n",
    "\n",
    "        part_1=(nu/(v_hat**.5+self.eps))\n",
    "        part_2=self.beta1*m_hat\n",
    "        part_3=(1-self.beta1)*ghat/(1-self.beta1**t)\n",
    "        step=part_1*(part_2+part_3) \n",
    "        return step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
