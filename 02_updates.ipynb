{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updates - Strategies for updating parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of various update rules that can be used by the `GradientDescent` optimizer are collected here. \n",
    "\n",
    "***\n",
    "\n",
    "`GradientDescent` uses two kinds of update rules\n",
    "\n",
    "1. Update steps (required): Determines how to update the model parameters based on the gradient estimate\n",
    "2. Acceptance criteria: Decides whether a parameter update should be accepted or rejected. `GradientDescent` always rejects parameter updates that yield an invalid cost (i.e. ```nan``` or ```inf```), but additional rules be provided.\n",
    " \n",
    "\n",
    "Many of the update steps are implemented based on [this review](https://arxiv.org/abs/1609.04747)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy\n",
    "import scipy\n",
    "from abc import ABC, abstractmethod, abstractproperty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class UpdateBase(ABC):\n",
    "    \"\"\"A helper class for constructing update rules\"\"\"\n",
    "#     @abstractmethod\n",
    "    #This is the workhorse of the class\n",
    "    \n",
    "    max_step=None\n",
    "\n",
    "    def evaluate(self,ghat, nu, t=0.):\n",
    "        step=self.step_update(ghat, nu, t)\n",
    "    \n",
    "        if self.max_step is not None:\n",
    "            #I would prefer these be called during creation\n",
    "            try:\n",
    "                self.max_step=float(self.max_step)\n",
    "                \n",
    "                assert self.max_step>0\n",
    "            except:\n",
    "                AssertionError(\"max_step must be a number greater than zero\")\n",
    "            \n",
    "            #If the proposed step is too large, rescale it\n",
    "    \n",
    "            max_proposal=numpy.max(numpy.abs(step))\n",
    "            if max_proposal>self.max_step:\n",
    "                step*=self.max_step/max_proposal\n",
    "            \n",
    "        return step\n",
    "    \n",
    "    @abstractmethod\n",
    "    #This is the workhorse of the class\n",
    "    def step_update(self): pass\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "## I should test that max step works properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`GradientDescent` expects update steps to be instances of a class with an ```evaluate``` method that takes as input a gradient estimate, a learning rate, and the current iteration (Comment: this seems like a suboptimal requirement), and returns a proposed parameter update. The reason update rules are required to be structured as classes is so that they can store any information they need to update the parameters. All update rules should return a numpy array of floats to be used as a step to update the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard SPSA step\n",
    "\n",
    "The default step used by Spall's classic SPSA algorithm is \n",
    "\n",
    "$$\\theta_{t+1}=\\theta_t - a_t \\hat{g}$$\n",
    "\n",
    "where $\\hat{g}$ is the gradient estimate and $a_t$ is the learning rate at iteration $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StandardSPSA(UpdateBase):\n",
    "\n",
    "    \"\"\"A standard gradient descent update.\"\"\"\n",
    "    def __init__(self, max_step):\n",
    "        self.max_step=max_step\n",
    "        pass\n",
    "\n",
    "    def step_update(self,ghat, nu, t=0. ):\n",
    "\n",
    "        return nu*ghat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ADAGRAD step update\n",
    "\n",
    "ADAGRAD\n",
    "\n",
    "The update rule is\n",
    "$$G_0=0$$\n",
    "$$G_t=G_{t-1}+(\\hat{g}_t)^2$$\n",
    "$$\\theta_{t+1}=\\theta_t - a_t \\frac{\\hat{g}_t}{\\sqrt{G_t}+\\epsilon} $$\n",
    "\n",
    "This step update requires a small term ```eps``` be defined to prevent divide be zero errors. By default this is set to ```1e-8```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ADAGRAD(UpdateBase):\n",
    "    \"\"\"The Adagrad gradient descent update.\"\"\"\n",
    "    def __init__(self, eps=1e-8, max_step=None):\n",
    "        self.max_step=max_step\n",
    "        self.eps=eps\n",
    "        self.G_t=None\n",
    "\n",
    "    def step_update(self,ghat, nu, t=0. ):\n",
    "        if self.G_t is None:\n",
    "            self.G_t=numpy.zeros(ghat.shape)\n",
    "        self.G_t+=ghat**2\n",
    "        return nu*ghat/(self.G_t+self.eps)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ADAM step update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Moment Estimation (Adam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ADAM(UpdateBase):\n",
    "    def __init__(self,beta1=.9, beta2=.999, eps=1e-8, max_step=None):\n",
    "        self.max_step=max_step\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.eps=eps\n",
    "        \n",
    "        self.m_t=[0.]\n",
    "        self.v_t=[0.]\n",
    "    def step_update(self,ghat, nu, t ):\n",
    "        #Update the gradient histories\n",
    "        self.m_t.append(self.beta1*self.m_t[-1]+(1-self.beta1)*ghat)\n",
    "        self.v_t.append(self.beta2*self.v_t[-1]+(1-self.beta2)*ghat**2)\n",
    "        \n",
    "        #compute the bias corrections\n",
    "        m_hat=self.m_t[-1]/(1.-self.beta1**t)\n",
    "        v_hat=self.v_t[-1]/(1.-self.beta2**t)\n",
    "        \n",
    "        #compute the proposed step\n",
    "        return nu*m_hat/(v_hat**.5+self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nestorov-accelerated ADAM step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NADAM(UpdateBase):\n",
    "    def __init__(self,beta1=.9, beta2=.999, eps=1e-8, max_step=None):\n",
    "        self.max_step=max_step\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.eps=eps\n",
    "        \n",
    "        self.m_t=[0.]\n",
    "        self.v_t=[0.]\n",
    "    def step_update(self,ghat, nu, t ):\n",
    "        #Update the gradient histories\n",
    "        self.m_t.append(self.beta1*self.m_t[-1]+(1-self.beta1)*ghat)\n",
    "        self.v_t.append(self.beta2*self.v_t[-1]+(1-self.beta2)*ghat**2)\n",
    "        \n",
    "\n",
    "        if t>=2:\n",
    "            m_hat=self.m_t[-1]/(1-self.beta1**(t-1))\n",
    "        else:\n",
    "            m_hat=0.\n",
    "        v_hat=self.v_t[-1]/(1-self.beta2**t)\n",
    "\n",
    "\n",
    "        part_1=(nu/(v_hat**.5+self.eps))\n",
    "        part_2=self.beta1*m_hat\n",
    "        part_3=(1-self.beta1)*ghat/(1-self.beta1**t)\n",
    "        step=part_1*(part_2+part_3) \n",
    "        return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance criteria (not really developed or used at the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "class AcceptanceBase(ABC):\n",
    "    \"\"\"A helper class for constructing update rules\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    #This initializes the class and is called when ```GradientDescent``` is instatiated\n",
    "    def initialize(self): pass\n",
    "    @abstractmethod\n",
    "    #This is the workhorse of the class\n",
    "    def evaluate(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "An acceptance criterion evaluates the cost after an update step and compares it to past evaluations of the cost function to decide whether it should be accepted or rejected. `GradientDescent` expects acceptance criteria to be objects of a class with two required methods:\n",
    "\n",
    "1. ```initialize```: This is called when a `GradientDescent` object is initialized and performs any routines needed to set-up the acceptance criteria.  For example, an acceptance criterion based on the amount of noise in the cost function might evaluate the cost function some number ot times and compute its standard deviation. This method will be passed the entire `GradientDescent` object, so it has access to all information stored therein.\n",
    "2. ```evaluate```: This "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BlockWithLocalResiduals(AcceptanceBase):\n",
    "    \"\"\"This acceptance rules computes a streaming linear regression\n",
    "    to estimate how the cost function has changed over the past k \n",
    "    iterations. It then requires that the a new update be \"\"\"\n",
    "    \n",
    "    def __init__(self,delta=2., window_size=100):\n",
    "        self.k=window_size\n",
    "        self.history=[]\n",
    "        self.times=[]\n",
    "        self.delta=delta\n",
    "        \n",
    "    def initialize(self, opt):\n",
    "        #Evaluate the cost function\n",
    "        self.SD=numpy.std([opt.cost.evaluate(opt.theta) for i in range(self.k) ])\n",
    "        self.history.append(opt.cost_history[-1])\n",
    "        self.times.append(0.)\n",
    "    def update_regression(self):\n",
    "        self.linreg=scipy.stats.linregress(self.times, self.history)\n",
    "        self.slope=self.linreg.slope\n",
    "        self.intercept=self.linreg.intercept\n",
    "    \n",
    "    \n",
    "    def evaluate(self, cost,t ):\n",
    "        accept=True\n",
    "        if t<self.k:\n",
    "            expected=self.history[-1]\n",
    "            if cost>expected+self.delta*self.SD: accept=False\n",
    "        if t>self.k:\n",
    "            self.update_regression()\n",
    "            expected=self.slope*numpy.array(self.times)+self.intercept\n",
    "            resid_sd=numpy.std(numpy.array(self.history)-self.times)\n",
    "            if cost>self.history[-1]+self.delta*resid_sd: accept=False\n",
    "        if accept==True:\n",
    "            self.history.append(cost)\n",
    "            self.times.append(t)\n",
    "            if t>self.k:\n",
    "                rem=self.history.pop(0)\n",
    "                rem=self.times.pop(0)\n",
    "        return (accept)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
