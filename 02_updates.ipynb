{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updates\n",
    "\n",
    "The various update rules that can be used by the ```GradientDescent``` optimizer are collected here. ```GradientDescent``` expects update rules to by instances of a class with an ```evaluate``` method that takes as input a gradient estimate, a learning rate, and the current iteration (Comment: this seems like a suboptimal requirement), and returns a proposed parameter update. The reason update rules are required to be structured as classes is so that they can store any information they need to update the parameters. All update rules should return a numpy array of floats to be used as a step to update the model parameters. \n",
    "\n",
    "Many of the update steps are implemented based on [this review](https://arxiv.org/abs/1609.04747)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy\n",
    "import scipy\n",
    "from abc import ABC, abstractmethod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class UpdateBase(ABC):\n",
    "    @abstractmethod\n",
    "    #This is the workhorse of the class\n",
    "    def evaluate(self): pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: I may want to do some restructuring and move the parameter stepsize, decay to the update rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The standard SPSA step\n",
    "\n",
    "The default step used by Spall's classic SPSA algorithm is \n",
    "\n",
    "$$\\theta_{t+1}=\\theta_t - a_t \\hat{g}$$\n",
    "\n",
    "where $\\hat{g}$ is the gradient estimate and $a_t$ is the learning rate at iteration $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StandardSPSA(UpdateBase):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self,ghat, nu, t=0. ):\n",
    "\n",
    "        return nu*ghat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ADAGRAD step update\n",
    "\n",
    "ADAGRAD\n",
    "\n",
    "The update rule is\n",
    "$$G_0=0$$\n",
    "$$G_t=G_{t-1}+(\\hat{g}_t)^2$$\n",
    "$$\\theta_{t+1}=\\theta_t - a_t \\frac{\\hat{g}_t}{\\sqrt{G_t}+\\epsilon} $$\n",
    "\n",
    "This step update requires a small term ```eps``` be defined to prevent divide be zero errors. By default this is set to ```1e-8```\n",
    "\n",
    "Note that typically, ADAGRAD is employed with a fixed learning rate. By default, however, ```GradientDescent``` uses a decreasing serir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ADAGRAD(UpdateBase):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        self.eps=eps\n",
    "        self.G_t=None\n",
    "\n",
    "    def evaluate(self,ghat, nu, t=0. ):\n",
    "        if self.G_t is None:\n",
    "            self.G_t=numpy.zeros(ghat.shape)\n",
    "        self.G_t+=ghat**2\n",
    "        return nu*ghat/(self.G_t+self.eps)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ADAM step update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Moment Estimation (Adam) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ADAM(UpdateBase):\n",
    "    def __init__(self,beta1=.9, beta2=.999, eps=1e-8):\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.eps=eps\n",
    "        \n",
    "        self.m_t=[0.]\n",
    "        self.v_t=[0.]\n",
    "    def evaluate(self,ghat, nu, t ):\n",
    "        #Update the gradient histories\n",
    "        self.m_t.append(self.beta1*self.m_t[-1]+(1-self.beta1)*ghat)\n",
    "        self.v_t.append(self.beta2*self.v_t[-1]+(1-self.beta2)*ghat**2)\n",
    "        \n",
    "        #compute the bias corrections\n",
    "        m_hat=self.m_t[-1]/(1.-self.beta1**t)\n",
    "        v_hat=self.v_t[-1]/(1.-self.beta2**t)\n",
    "        \n",
    "        #compute the proposed step\n",
    "        return nu*m_hat/(v_hat**.5+self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nestorov-accelerated ADAM step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NADAM(UpdateBase):\n",
    "    def __init__(self,beta1=.9, beta2=.999, eps=1e-8):\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.eps=eps\n",
    "        \n",
    "        self.m_t=[0.]\n",
    "        self.v_t=[0.]\n",
    "    def evaluate(self,ghat, nu, t ):\n",
    "        #Update the gradient histories\n",
    "        self.m_t.append(self.beta1*self.m_t[-1]+(1-self.beta1)*ghat)\n",
    "        self.v_t.append(self.beta2*self.v_t[-1]+(1-self.beta2)*ghat**2)\n",
    "        \n",
    "\n",
    "        if t>=2:\n",
    "            m_hat=self.m_t[-1]/(1-self.beta1**(t-1))\n",
    "        else:\n",
    "            m_hat=0.\n",
    "        v_hat=self.v_t[-1]/(1-self.beta2**t)\n",
    "\n",
    "\n",
    "        part_1=(nu/(v_hat**.5+self.eps))\n",
    "        part_2=self.beta1*m_hat\n",
    "        part_3=(1-self.beta1)*ghat/(1-self.beta1**t)\n",
    "        step=part_1*(part_2+part_3) \n",
    "        return step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
