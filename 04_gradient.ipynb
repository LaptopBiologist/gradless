{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient\n",
    "\n",
    "Spall's simultaneous perturbation stochastic approximation (SPSA) of the gradient provides an efficient means to approximate the gradient of high-dimensional models, even when only noisy evaluations of the objective function are available. This is in constrast to more typical applications of stochastic gradient descent, where the noisiness of the gradient comes not from the objective function itself, but rather from evaluating the gradient on subsets of the data. \n",
    "\n",
    "### Approximating the gradient with SPSA \n",
    "\n",
    "The general idea of SPSA is reasonably straightforward. Given a step size $c_k$ and a vector of perturbations $\\delta$, we first generate forward and backward perturbations all model parameters simultaneously\n",
    "\n",
    "$$\\theta^+ = \\theta + c_k \\delta$$\n",
    "$$\\theta^- = \\theta - c_k \\delta$$\n",
    "\n",
    "The perturbation, $\\delta$ is often sampled from a shifted and rescaled Bernoulli distribution as follows:\n",
    "\n",
    "$$b_1, b_2,..., b_m \\sim Bernoulli(p=.5)$$\n",
    "$$\\delta_i = 2b_i -1$$\n",
    "\n",
    "where $\\delta_i$ is the direction in which the $i$-th model parameter will be moved in the forward perturbation.\n",
    "\n",
    "We then evaluate the cost function at the two perturbed parameters\n",
    "\n",
    "$$y^+ = F(\\theta^+, X)$$\n",
    "$$y^- = F(\\theta^-, X)$$\n",
    "\n",
    "Note that cost function itself may be noisy.\n",
    "\n",
    "We approximate the gradient as the slope of the line between the points $(\\theta^+, y^+)$ and $(\\theta^-, y^-)$:\n",
    "\n",
    "$$\\hat{g}= \\frac{y^+-y^-}{\\theta^+ - \\theta^-}= \\frac{y^+-y^-}{2 c_k \\delta}$$\n",
    "\n",
    "\n",
    "[place holder text]\n",
    "\n",
    "### Perturbing subset of parameters\n",
    "\n",
    "In some models, it might be desirable to evaluate the gradient separately for difference subsets of parameters. For example, in variational inference, the means of the posterior approximation have a much stronger impact on the loss function than the standard deviations do. In that case, perturbing all parameters at once is likely to pick up the impact of perturbing the means on the gradient, but perhaps not the standard deviations.\n",
    "\n",
    "The ```param_labels``` option permits to the gradient approximation to be evaluated separately for subsets of parameters. If, for example. ```param_labels=[0,0,0,1,1,1]```, then the gradient will be approximated in two steps. The gradient will be estimated first for the three first parameters, perturbing them while holding the other parameters constant. Then the parameters labelled ```1``` will be perturbed, while all others are held constant. The cost of doing this is the number of cost function evaluations increases from $2$ to $2n$, where is $n$ number of parameter subset to be evaluated separately. \n",
    "\n",
    "### Averaging multiple gradient approximations\n",
    "\n",
    "Rather than approximating the gradient from a single perturbation, the parameter ```gradient_reps``` can be employed to instead return the average of multiple gradient evaluations. This may lead to more efficient parameter updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import numpy\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SPSAGradient():\n",
    "    def __init__(self, param_subsets=None):\n",
    "        self.param_subsets=param_subsets\n",
    "        if self.param_subsets is not None:\n",
    "            self.param_subsets=numpy.array(self.param_subsets)\n",
    "            self.subsets=set(list(param_subsets))\n",
    "        \n",
    "    \n",
    "    def evaluate(self, cost, theta, c_k, gradient_reps=1):\n",
    "#         assert len(theta)==len(self.)\n",
    "        #If no subsets were defined, then now we'll define all model parameters as one set\n",
    "        if self.param_subsets is None:\n",
    "            self.param_subsets=numpy.zeros(theta.shape[0])\n",
    "            self.subsets=set(list(self.param_subsets))\n",
    "        #evaluate the gradient separately for different groups of parameters\n",
    "        grad_list=[]\n",
    "        for rep in range(gradient_reps):\n",
    "            \n",
    "            ghat=numpy.zeros(theta.shape)\n",
    "            for s in self.subsets:\n",
    "                param_filter=self.param_subsets==s\n",
    "                ghat+=self.SPSA(cost, theta, c_k, param_filter)\n",
    "            grad_list.append(ghat)\n",
    "        if gradient_reps==1:\n",
    "            return grad_list[0]\n",
    "        else: #We need to average\n",
    "            return numpy.mean(grad_list,0)\n",
    "        \n",
    "    def SPSA(self, cost, theta, ck, param_ind):\n",
    "        \"\"\" Inputs:\n",
    "            cost - a function that takes model parameters and data as inputs\n",
    "                    and returns a single float\n",
    "            data - the data the model is being fit to\n",
    "            theta - a set model parameters\n",
    "            ck - the step size to be used during perturbation of the model parameters\n",
    "\n",
    "            Outputs:\n",
    "            An estimate of the gradient\n",
    "        \"\"\"\n",
    "        #Draw the perturbation\n",
    "\n",
    "        delta=2*scipy.stats.bernoulli.rvs(p=.5,size=theta.shape[0])-1\n",
    "        #hold delta constant for the parameters not under consideration\n",
    "        delta[~param_ind]=0.\n",
    "        #Perturb the parameters forwards and backwards\n",
    "        thetaplus=theta+ck*delta\n",
    "        thetaminus=theta-ck*delta\n",
    "\n",
    "        #Evaluate the objective after the perturbations\n",
    "        yplus=cost.evaluate(thetaplus)\n",
    "        yminus=cost.evaluate(thetaminus)\n",
    "\n",
    "        #Compute the slope across the perturbation\n",
    "\n",
    "        ghat=(yplus-yminus)/(2*ck*delta)\n",
    "\n",
    "        ghat[~param_ind]=0\n",
    "        return ghat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradless.costs import CustomCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
