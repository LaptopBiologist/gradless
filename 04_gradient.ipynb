{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient - Approximating the gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A collection of classes and functions used to approximate the gradient.\n",
    "***\n",
    "## Background\n",
    "\n",
    "Spall's simultaneous perturbation stochastic approximation (SPSA) of the gradient provides an efficient means to approximate the gradient of high-dimensional models, even when only noisy evaluations of the objective function are available. This is in constrast to more typical applications of stochastic gradient descent, where the noisiness of the gradient comes not from the objective function itself, but rather from evaluating the gradient on subsets of the data. \n",
    "\n",
    "### Approximating the gradient with SPSA \n",
    "\n",
    "The general idea of SPSA is reasonably straightforward. Given a step size $c_t$ and a vector of perturbations $\\delta$, we first generate forward and backward perturbations all model parameters simultaneously\n",
    "\n",
    "$$\\theta^+ = \\theta + c_t \\delta$$\n",
    "$$\\theta^- = \\theta - c_t \\delta$$\n",
    "\n",
    "The perturbation, $\\delta$ is often sampled from a shifted and rescaled Bernoulli distribution as follows:\n",
    "\n",
    "$$b_1, b_2,..., b_m \\sim Bernoulli(p=.5)$$\n",
    "$$\\delta_i = 2b_i -1$$\n",
    "\n",
    "where $\\delta_i$ is the direction in which the $i$-th model parameter will be moved in the forward perturbation.\n",
    "\n",
    "We then evaluate the cost function $F(\\theta, X)$ at the two perturbed parameters\n",
    "\n",
    "$$y^+ = F(\\theta^+, X)$$\n",
    "$$y^- = F(\\theta^-, X)$$\n",
    "\n",
    "The gradient is approximated as the slope of the line between the points $(\\theta^+, y^+)$ and $(\\theta^-, y^-)$:\n",
    "\n",
    "$$\\hat{g}= \\frac{y^+-y^-}{\\theta^+ - \\theta^-}= \\frac{y^+-y^-}{2 c_t \\delta}$$\n",
    "\n",
    "A major advantage of this approximation is that in its simplest form, only two evaluations of the cost function are required, regardless of the dimensionality of the model. This is in constrast to the [finite-differences approximation]() which requires each model parameter be perturbed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import numpy\n",
    "import scipy\n",
    "from abc import ABC, abstractmethod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class GradientBase(ABC):\n",
    "    \"\"\"A helper class that provides a standard means to create\n",
    "    classes to provide gradients or their approximations to GradientDescent.\"\"\"\n",
    "    @abstractmethod\n",
    "    #This is the workhorse of the class\n",
    "    def evaluate(self): pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "```GradientDescent``` must be passed an object with a method called ```.evaluate()```. This should store as an attribute the cost function to be evaluated and take the following inputs:\n",
    "\n",
    "1. theta - A 1-D numpy array of model parameters\n",
    "2. c_k - A step size that may be used in the gradient evaluation\n",
    "3. gradient_reps - The number of times to evaluate the gradient (multiple evaluations will be averaged)\n",
    "4. update_rvs - Whether regenerated random variables stored in the cost function after each gradient evaluation\n",
    "\n",
    "It should return a vector of the same length as ```theta``` containing an estimate of the cost function's gradient at ```theta```.\n",
    "\n",
    "Any approach to gradient evaluation will require the first argument, ```theta```. The latter three are only necessary when using an approximation of the gradient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SPSAGradient(GradientBase):\n",
    "    \"\"\"A class for computing the SPSA gradient estimate.\"\"\"\n",
    "    def __init__(self, param_subsets=None,fraction=None, cost=None):\n",
    "        self.cost=cost\n",
    "        self.param_subsets=param_subsets\n",
    "        if self.param_subsets is not None:\n",
    "            self.param_subsets=numpy.array(self.param_subsets)\n",
    "            self.subsets=set(list(param_subsets))\n",
    "        \n",
    "    def set_cost(self, cost):\n",
    "        self.cost=cost\n",
    "        \n",
    "    def evaluate(self, theta, c_k, gradient_reps=1, update_rvs=False):\n",
    "        \"\"\"Inputs\n",
    "        1. theta - A 1-D numpy array of model parameters\n",
    "        2. c_k - A step size that may be used in the gradient evaluation\n",
    "        3. gradient_reps - The number of times to evaluate the gradient \n",
    "            (multiple evaluations will be averaged)\n",
    "        4. update_rvs - Whether regenerated random variables stored in \n",
    "            the cost function after each gradient evaluation\n",
    "            \n",
    "            Returns an array gradient estimates the same size as theta\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#         assert len(theta)==len(self.)\n",
    "        #If no subsets were defined, then now we'll define all model parameters as one set\n",
    "        assert self.cost is not None\n",
    "        \n",
    "        if self.param_subsets is None:\n",
    "            self.param_subsets=numpy.zeros(theta.shape[0])\n",
    "            self.subsets=set(list(self.param_subsets))\n",
    "        #evaluate the gradient separately for different groups of parameters\n",
    "        grad_list=[]\n",
    "        for rep in range(gradient_reps):\n",
    "            if update_rvs==True:     #Regenerate the random numbers in the cost with each gradient\n",
    "                self.cost.sample_rvs()\n",
    "            ghat=numpy.zeros(theta.shape)\n",
    "            for s in self.subsets:\n",
    "                \n",
    "                param_filter=self.param_subsets==s\n",
    "\n",
    "                ghat+=self.SPSA( theta, c_k, param_filter)\n",
    "            grad_list.append(ghat)\n",
    "        if gradient_reps==1:\n",
    "            return grad_list[0]\n",
    "        else: #We need to average\n",
    "#             print (grad_list)\n",
    "#             print ( numpy.mean(grad_list,0))\n",
    "#             print (jabber)\n",
    "            return numpy.mean(grad_list,0)\n",
    "        \n",
    "    def SPSA(self, theta, ck, param_ind):\n",
    "        \"\"\" Inputs:\n",
    "            cost - a function that takes model parameters and data as inputs\n",
    "                    and returns a single float\n",
    "            data - the data the model is being fit to\n",
    "            theta - a set model parameters\n",
    "            ck - the step size to be used during perturbation of the model parameters\n",
    "\n",
    "            Outputs:\n",
    "            An estimate of the gradient\n",
    "        \"\"\"\n",
    "        #Draw the perturbation\n",
    "\n",
    "        delta=2.*scipy.stats.bernoulli.rvs(p=.5,size=theta.shape[0])-1.\n",
    "        #hold delta constant for the parameters not under consideration\n",
    "        delta[~param_ind]=0.\n",
    "        #Perturb the parameters forwards and backwards\n",
    "        thetaplus=theta+ck*delta\n",
    "        thetaminus=theta-ck*delta\n",
    "\n",
    "        #Evaluate the objective after the perturbations\n",
    "        yplus=self.cost.evaluate(thetaplus)\n",
    "        yminus=self.cost.evaluate(thetaminus)\n",
    "\n",
    "        #Compute the slope across the perturbation\n",
    "\n",
    "        ghat=(yplus-yminus)/(2*ck*delta)\n",
    "\n",
    "        ghat[~param_ind]=0\n",
    "        return ghat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The `SPSAGradient` class is used by `GradientDescent` to approximate the gradient of an objective function, which can then be used to update model parameters.\n",
    "\n",
    "This takes two arguments, both of which are optional:\n",
    "\n",
    "1. ```param_subsets``` (optional) - A list or array of labels that defines groups of parameters. For example, \\[0,0,0,1,1,1] defines the first three model parameters as group 0 and the last three as belong to group 1.  \n",
    "\n",
    "2. ```cost``` (optional) - The cost function used in the gradient evaluation. When passing an instance of the `SPSAGradient` class to the `GradientDescent` optimizer, this should be left undefined. The `GradientDescent` object will automatically add the cost function being optimized to the `SPSAGradient` if its cost function has not been defined. \n",
    "\n",
    "\n",
    "\n",
    "#### Perturbing subset of parameters\n",
    "\n",
    "In some models, it might be desirable to evaluate the gradient separately for different subsets of parameters. For example, in variational inference, the means of the posterior approximation have a much stronger impact on the loss function than the standard deviations do. In that case, perturbing all parameters at once is likely to pick up the impact of perturbing the means on the gradient, but perhaps not the standard deviations.\n",
    "\n",
    "The ```param_labels``` option permits to the gradient approximation to be evaluated separately for subsets of parameters. If, for example. ```param_labels=[0,0,0,1,1,1]```, then the gradient will be approximated in two steps. The gradient will be estimated first for the three first parameters, perturbing them while holding the other parameters constant. Then the parameters labelled ```1``` will be perturbed, while all others are held constant. The cost of doing this is the number of cost function evaluations increases from $2$ to $2n$, where is $n$ number of parameter subset to be evaluated separately. \n",
    "\n",
    "#### Averaging multiple gradient approximations\n",
    "\n",
    "By default calling ```evaluate``` approximates the gradient from a single forward and backward perturbation. The argument ```gradient_reps``` can instead be set to an integer value greater than 1, to instead return the average of multiple gradient evaluations. If ```gradient_reps``` is set to $r$, ```evaluate``` will return the average of $r$ gradient approximations. This may lead to faster convergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
