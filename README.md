# gradless



This is an implementation of gradient descent designed to work without access to the exact gradient. To deal with this problem, it uses James Spall's [simultaneous perturbation stochastic approximation (SPSA)](https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF) to replace the missing gradient. 

SPSA is particularly useful for optimization problems where the objective function itself is noisy, such that the exact gradient cannot be evaluated. For example, if the model at hand is evaluated by simulations rather than exact computations. This is in contrast to more typical applications of stochastic gradient descent, where the gradient can be computed, but noise is introduced through subsampling of the data (e.g. minibatching) or by Monte Carlo integration (e.g. in variational inference). 

## Goals

This is a library I'm developing for use in a research problem of personal interest involving a noisy objective function. As such, it is both still in development and likely to be tailored toward my needs. My principle aim in writing this library is to have a structured, modular, and easy-to-modify framework with a defined API, separate from the actual model I'm working on so that the problem and the tools being applied to the problem are kept separate, hopefully making my research workflow cleaner and more efficient.

I decided to use this as an opportunity to try incorporating literate programming, as accomplished by [```nbdev```](https://github.com/fastai/nbdev), into my development workflow. I've been relying heavily on Jupyter notebooks to document my analyses by interspersing code and narrative. Extending this programming style to script development seemed like a good idea. In brief, ```nbdev``` provides a set of tools for organizing a Python project as set of Jupyter notebooks. All of the actual development takes place in those Jupyter notebooks, rather than an IDE. ```nbdev``` then extracts all code in cells marked with the header ```#export``` to a set of Python scripts and generates documentation from the markdown cells and code cells not labelled ```#export``` or ```#hide```. I am very much enjoying ```nbdev``` and have so far found it a painless and natural approach to writing scripts, and actually getting documentation done (I know I sound like a shill, but it really feels like a phenomenal and transformative tool). This scripts, documentation, and webpage found [here](laptopbiologist.github.io/gradless) were all automatically generated by ```nbdev``` from the ```*.ipynb``` notebooks found in the repositories main directory.  

As this is still in development and geared for personal use, I can't make any general guarantees about its performance or behavior. So if you must use it, use with caution and skepticism.


## To do

* Incorporate just-in-time compilation with JAX to speed up time-consuming functions
* Define a class to organize minibatching. Probably class that wraps datasets along with instructions for how to minibatch the data. This would be called by the ```Model``` class when it evaluates the cost function with data. May need to modify ```Model``` a bit in terms of how it stores and uses ```self.data```.
* Implement some procedures for smart hyperparameter choice

## Example usage

Okay, let's try out a few toy examples, just to demonstrate how the API works. 

First we'll import the packages that we need.



```python
import numpy
import scipy
from matplotlib import pyplot
import seaborn

from tqdm import tqdm

from gradless import optimizers, costs, gradient, updates
```

### Simple linear regression

Let's say we are interested in the mean-squared error of a simple linear regression.



Now let's generate 200 data points from a simple linear relationship, with a slope of 2 and an intercept of 5:

{% raw %}
$$x \sim normal(0, 5)$$
{% endraw %}

{% raw %}
$$\epsilon \sim normal(0,2)$$
{% endraw %}

{% raw %}
$$y=2x+5+\epsilon$$
{% endraw %}



```python
x=scipy.stats.norm.rvs(0, 5, size=200)
err=scipy.stats.norm.rvs(0, 2, size=200)
slope=2
intercept=5
y=x*slope+intercept +err
pyplot.scatter(x,y)
pyplot.ylabel('y')
pyplot.xlabel('x')

```




    Text(0.5, 0, 'x')




![png](docs/images/output_5_1.png)


Let's organized this data as a dictionary

```python
data={'x':x,
     'y':y}
```

Now we need a function that takes a vector of parameter values and data and uses these to returns a float. Note that it does not matter how the data is organized, so long as the function can interpret it internally.


```python
def MSE(theta, data):
    x,y=data['x'], data['y']
    y_pred=theta[0]*x+theta[1]
    return numpy.mean((y-y_pred)**2)
```

Okay, now we'll wrap the cost function and the data in the ```Model``` class

```python
mse_cost=costs.Model(cost=MSE, data=data)
```

We'll fit this using the standard gradient descent algorithm described [hera](https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF). To do this, we're going to construct an instance of the ```GradientDescent``` class, passing it the model, an initial guess, and an update rule.

First, we'll choose an update rule from the ```updates``` submodule, creating an instance of the ```StandardSPSA``` class.

```python
update_rule=updates.StandardSPSA()
```

Now we'll create an instance of the ```GradientDescent``` optimizer.

```python
starts=numpy.array([-9,-9]) #Here's our initial guess
opt_vanilla=optimizers.GradientDescent(x_0 = starts,
                                       cost = mse_cost,
                                       update = update_rule,
                                       gradient=gradient.SPSAGradient(numpy.array([0,0])),
                                       param_stepsize=1, param_stepdecay=0.5, param_decay_offset=0, 
                                       grad_stepsize=1, grad_stepdecay=.3, )
```

Note that there are additional parameters that need to be defined. The arguments beginning with ```param``` all control the much the parameters will be updated at each iteration and how this decays over time. The two arguments beginning with ```grad``` control how much the model parameters will be perturbed during the gradient approximation. It may be necessary to tune these parameters to work with a given model.

Now we can optimize the model by iteratively calling the ```update_params``` method of the ```GradientDescent``` instance. Note that we can use the ```tqdm``` library to create a progress bar for the fit

```python
opt_vanilla.update_params(gradient_reps=100, max_step=1)
for i in tqdm(range(10000)):
    opt_vanilla.update_params(gradient_reps=3, max_step=1)
#     elbo.sample_rvs()
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-9-ddb2d21a4ae9> in <module>
    ----> 1 opt_vanilla.update_params(gradient_reps=100, max_step=1)
          2 for i in tqdm(range(10000)):
          3     opt_vanilla.update_params(gradient_reps=3, max_step=1)
          4 #     elbo.sample_rvs()


    ~/projects/gradless/gradless/optimizers.py in update_params(self, gradient_reps, block_val, update_rvs, max_step)
         54 
         55         ### update the parameters
    ---> 56         new_theta=self.theta-step
         57         new_cost=self.cost.evaluate(new_theta)
         58 


    ValueError: operands could not be broadcast together with shapes (2,) (100,) 


The ```max_step``` argument in ```update_params``` can be used to limit how much parameters are allowed to be updated in each iteration, which can served as heuristic to limit divergences, especially in the early iterations where the learning rate is high.

We could also have employed other gradient descent update rules, for example Nestorov-accelerated Adam, which uses the history of prior gradients to (hopefully) more informed updates.

```python
update_rule=updates.NADAM( beta1=.9)
opt_NADAM=optimizers.GradientDescent(starts,mse_cost,update_rule,gradient.SPSAGradient(numpy.array([0,0])),param_stepsize=1, param_stepdecay=0.5, param_decay_offset=0, 
                 grad_stepsize=1, grad_stepdecay=.2, )
```

```python
opt_NADAM.update_params(gradient_reps=100, max_step=1)
for i in tqdm(range(10000)):
    opt_NADAM.update_params(gradient_reps=3, max_step=1)
#     elbo.sample_rvs()
```

```python
X,Y,Z=[],[],[]
for slope in numpy.arange(-15, 15,.2):

    for intercept in numpy.arange(-15, 15,.2):
        Y.append(intercept)
        X.append(slope)
        Z.append(mse_cost.evaluate([slope,intercept]))

```

```python
pyplot.figure(figsize=(15,6))
pyplot.subplot(121)
theta_hist=numpy.array(opt_vanilla.theta_hist)
sc=pyplot.scatter(X,Y, c=numpy.log(Z), zorder=-1)
pyplot.scatter(starts[0], starts[1], c='white', edgecolor='black', s=100)
pyplot.plot(theta_hist[:,0],theta_hist[:,1], c='red', lw=2, zorder=0)
pyplot.scatter(2, 5, marker='x', c='white', s=400)
pyplot.scatter(theta_hist[-1,0],theta_hist[-1,1], marker='o',edgecolor='red', c='black', s=80)

cbar=pyplot.colorbar(sc)
cbar.set_label('log (MSE)', size=14)
pyplot.ylabel('Intercept', size=14)
pyplot.xlabel('Slope', size=14)
pyplot.xlim(-15,15)
pyplot.ylim(-15,15)
pyplot.title('Standard SPSA gradient descent')

pyplot.subplot(122)
theta_hist=numpy.array(opt_NADAM.theta_hist)
sc=pyplot.scatter(X,Y, c=numpy.log(Z), zorder=-1)
pyplot.scatter(starts[0], starts[1], c='white', edgecolor='black', s=100)
pyplot.plot(theta_hist[:,0],theta_hist[:,1], c='red', lw=2, zorder=0)
pyplot.scatter(2, 5, marker='x', c='white', s=400)
pyplot.scatter(theta_hist[-1,0],theta_hist[-1,1], marker='o',edgecolor='red', c='black', s=80)

cbar=pyplot.colorbar(sc)
cbar.set_label('log (MSE)', size=14)
pyplot.ylabel('Intercept', size=14)
pyplot.xlabel('Slope', size=14)
pyplot.xlim(-15,15)
pyplot.ylim(-15,15)
pyplot.title('Nesterov-accelerated Adam with SPSA')
```

```python
pyplot.plot(opt_vanilla.cost_history,c='blue', label='StandardSPSA')
pyplot.plot(opt_NADAM.cost_history,c='red', label='NADAM')
pyplot.xscale('log')
pyplot.ylabel('MSE')
pyplot.xlabel('Iteration')
pyplot.yscale('log')
pyplot.legend()
```

### Fitting the same regression with an intentionally bad loss function

Let's say instead that we decided to optimize this is in a pretty nonsensical fashion. Instead of just computing the loss as

{% raw %}
$$\sum_i (y_i - \hat{y}_i)^2$$
{% endraw %}

we'll add a considerable amount of noise to each prediction and compute it as

{% raw %}
$$\epsilon_1,...,\epsilon_n \sim Normal(\mu=0,\sigma =30)$$
{% endraw %}
{% raw %}
$$\sum_i (y_i - (\hat{y}_i + \epsilon_i) )^2$$
{% endraw %}

This is essentially simulating a noisier dataset from the proposed parameters and the computing the distance between the y-values in the real and simulated data. 

```python
def MSE_simulated_data(theta, data):
    x,y=data['x'], data['y']
    y_pred=scipy.stats.norm.rvs( theta[0]*x+theta[1], 30.)
    
    return numpy.mean((y-y_pred)**2)
```

```python


```

Then we instatiate an instance of the ```GradientDescent``` class with the model, the initial guess, and the update rule.

```python

mse_sim_cost=costs.Model(MSE_simulated_data, data)
starts=numpy.array([-9,-9])
update_rule=updates.StandardSPSA()
opt_vanilla=optimizers.GradientDescent(starts,mse_sim_cost,update_rule,
                                       gradient.SPSAGradient(numpy.array([0,0])),
                                       param_stepsize=.2, param_stepdecay=0.3, param_decay_offset=0, 
                                       grad_stepsize=1, grad_stepdecay=.2, )
```

```python
opt_vanilla.update_params(gradient_reps=100, max_step=.1)
for i in tqdm(range(50000)):
    opt_vanilla.update_params(gradient_reps=3, max_step=.1)
#     elbo.sample_rvs()
```

The ```max_step``` argument in ```update_params``` can be used to limit how much parameters are allowed to be updated in each iteration, which can served as heuristic to limit divergences, especially in the early iterations where the learning rate is high.

```python

mse_sim_cost=costs.Model(MSE_simulated_data, data)
starts=numpy.array([-9,-9])
update_rule=updates.NADAM( beta1=.9)
opt_NADAM=optimizers.GradientDescent(starts,mse_sim_cost,update_rule,gradient.SPSAGradient(numpy.array([0,0])),param_stepsize=.2, param_stepdecay=0.3, param_decay_offset=0, 
                 grad_stepsize=1, grad_stepdecay=.2, )
```

```python
opt_NADAM.update_params(gradient_reps=100, max_step=.1)
for i in tqdm(range(50000)):
    opt_NADAM.update_params(gradient_reps=3, max_step=.1)
#     elbo.sample_rvs()
```

```python
X,Y,Z=[],[],[]
for slope in numpy.arange(-15, 15,.2):

    for intercept in numpy.arange(-15, 15,.2):
        Y.append(intercept)
        X.append(slope)
        Z.append(mse_sim_cost.evaluate([slope,intercept]))

```

```python
pyplot.figure(figsize=(15,6))
pyplot.subplot(121)
theta_hist=numpy.array(opt_vanilla.theta_hist)
sc=pyplot.scatter(X,Y, c=numpy.log(Z), zorder=-1)
pyplot.scatter(starts[0], starts[1], c='white', edgecolor='black', s=100)
pyplot.plot(theta_hist[:,0],theta_hist[:,1], c='red', lw=2, zorder=0)
pyplot.scatter(2, 5, marker='x', c='white', s=400)
pyplot.scatter(theta_hist[-1,0],theta_hist[-1,1], marker='o',edgecolor='red', c='black', s=80)

cbar=pyplot.colorbar(sc)
cbar.set_label('log (MSE)', size=14)
pyplot.ylabel('Intercept', size=14)
pyplot.xlabel('Slope', size=14)
pyplot.xlim(-15,15)
pyplot.ylim(-15,15)
pyplot.title('Standard SPSA gradient descent')

pyplot.subplot(122)
theta_hist=numpy.array(opt_NADAM.theta_hist)
sc=pyplot.scatter(X,Y, c=numpy.log(Z), zorder=-1)
pyplot.scatter(starts[0], starts[1], c='white', edgecolor='black', s=100)
pyplot.plot(theta_hist[:,0],theta_hist[:,1], c='red', lw=2, zorder=0)
pyplot.scatter(2, 5, marker='x', c='white', s=400)
pyplot.scatter(theta_hist[-1,0],theta_hist[-1,1], marker='o',edgecolor='red', c='black', s=80)

cbar=pyplot.colorbar(sc)
cbar.set_label('log (MSE)', size=14)
pyplot.ylabel('Intercept', size=14)
pyplot.xlabel('Slope', size=14)
pyplot.xlim(-15,15)
pyplot.ylim(-15,15)
pyplot.title('Nesterov-accelerated Adam with SPSA')
# pyplot.figure(figsize=(8,8))
# sc=pyplot.scatter(X,Y, c=numpy.log(Z), zorder=-1, s=20)
# pyplot.scatter(starts[0], starts[1], c='white', edgecolor='black', s=100)
# pyplot.plot(theta_hist[:,0],theta_hist[:,1], c='red', lw=2, zorder=0)
# pyplot.scatter(theta_hist[-1,0],theta_hist[-1,1], c='red', lw=2, zorder=0)
# pyplot.scatter(2, 5, marker='x', c='white', s=200)

# cbar=pyplot.colorbar(sc)
# cbar.set_label('log (MSE)', size=14)
# pyplot.ylabel('Intercept', size=14)
# pyplot.xlabel('Slope', size=14)
# pyplot.xlim(-15,15)
# pyplot.ylim(-15,15)
```

It's likely that choosing different parameters for the step size of the standard SPSA gradient descent would lead to better performance.

### High dimensional model

```python
ndim=100
means=scipy.stats.norm.rvs(0, 10, size=ndim)
sd=2.**scipy.stats.norm.rvs(3,1.5 , size=ndim)
data=scipy.stats.norm.rvs(means, sd, size=(100,ndim))
true_param=numpy.zeros(2*ndim)
true_param[::2]=means
true_param[1::2]=numpy.log2(sd)
```

```python
starts=scipy.stats.norm.rvs(0,3, size=means.shape[0]+sd.shape[0])
```

```python
def evidence(theta, data):
    #priors
    mu=theta[::2]
    sd=theta[1::2]

    mu_prior=scipy.stats.norm.logpdf(mu, 0, 10).sum()
    sd_prior=scipy.stats.norm.logpdf(sd, 0, 10).sum()
    
    loglk=scipy.stats.norm.logpdf(data, mu,2.**(sd)).sum()
    return -(mu_prior+sd_prior+loglk)
```

To make this noisy, before evaluating the parameters, I'm going to add some noise drawn from $Normal(0,.5)$ to them.

```python
def noisy_evidence(theta, data):
    #priors
    mu=theta[::2]
    sd=theta[1::2]
    
    mu_noise=scipy.stats.norm.rvs(0,.5, size=mu.shape)
    sd_noise=scipy.stats.norm.rvs(0,.5, size=sd.shape)
    
    mu_prior=scipy.stats.norm.logpdf(mu, 0, 10).sum()
    sd_prior=scipy.stats.norm.logpdf(sd, 0, 10).sum()
    
    loglk=scipy.stats.norm.logpdf(data, mu+mu_noise,2.**(sd+sd_noise)).sum()
    return -(mu_prior+sd_prior+loglk)
```

```python
print (evidence(starts, data))
```

```python
seaborn.distplot([noisy_evidence(true_param, data) for i in range(200)])
pyplot.scatter(evidence(true_param, data),0, c='r')
```

```python
print (starts.shape)
```

```python
model=costs.Model(noisy_evidence, data)

update_rule=updates.NADAM( beta1=.9)
opt=optimizers.GradientDescent(starts,model,update_rule,gradient.SPSAGradient([0,1]*ndim),
                               param_stepsize = 5, param_stepdecay = .2, param_decay_offset = 20, 
                               grad_stepsize = 2, grad_stepdecay = .2, )
```

```python
# grad_test=gradient.SPSAGradient()
```

```python
# grad_test.evaluate(post_proxy, starts,1)
```

```python
from tqdm import tqdm
```

```python

opt.update_params(gradient_reps=100, max_step=1)
for i in tqdm(range(10000)):
    opt.update_params(gradient_reps=1, block_val=1.2, max_step=1 )
#     elbo.sample_rvs()
```

```python
for i in tqdm(range(20000)):
    opt.update_params(gradient_reps=1, block_val=1.2, max_step=1 )
#     elbo.sample_rvs()
```

```python
len(opt.theta_hist)
```

```python
pyplot.plot(numpy.log10(opt.cost_history))
```

```python
theta_hist=numpy.array(opt.theta_hist)
```

```python
true_param.shape
```

```python
pyplot.plot(theta_hist[-10000:,1], theta_hist[-10000:,2], zorder=-1)
pyplot.scatter(true_param[1], true_param[2], c='r')
```

```python
theta_hist[-1,:]- theta_hist[-2,:]
```

```python
pyplot.scatter(theta_hist[-1,::2], means)
```

```python
numpy.argmax(theta_hist[-1,::2]-means)
```

```python
# data[:,13]
```

```python
pyplot.scatter(theta_hist[-1,1::2], numpy.log2(sd))
```

Let's compare this to the posterior means, inferred using PyMC3. I'll set up the model and draw samples from the posterior using the No-U-Turn Sampler (NUTS).

```python
import pymc3 as pm
with pm.Model() as model:
    mu=pm.Normal('mu',0,10, shape=ndim)
    std=2.**pm.Normal('sd',0,10, shape=ndim)
    obs=pm.Normal('obs',mu, std, observed=data)
with model:
    trace=pm.sample()
```

Let's compute the posterior means

```python
mean_mu=trace['mu'].mean(0)
sd_mu=trace['sd'].mean(0)

```

```python
And now well plot the 
```

```python
pyplot.scatter(theta_hist[-1,::2], (mean_mu))
pyplot.plot([-30,20],[-30,20], color='black')
```

```python
pyplot.scatter(theta_hist[-1,1::2], (sd_mu))
pyplot.plot([-1,7],[-1,7], color='black')
```

```python
pyplot.plot(numpy.log(opt.cost_history))
```
