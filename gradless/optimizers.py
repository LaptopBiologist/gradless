# AUTOGENERATED! DO NOT EDIT! File to edit: 01_optimizers.ipynb (unless otherwise specified).

__all__ = ['GradientDescent']

# Cell
import numpy
import scipy

# Cell
class GradientDescent():
    def __init__(self,x_0, cost, update, gradient=SPSAGradient(),
                 param_stepsize=1, param_stepdecay=.4, param_decay_offset=0,
                 grad_stepsize=1, grad_stepdecay=.2, ):
        self.cost=cost
        self.update=update
        self.gradient=gradient
        self.param_stepsize=param_stepsize
        self.param_stepdecay=param_stepdecay
        self.param_decay_offset=param_decay_offset
        self.grad_stepsize=grad_stepsize
        self.grad_stepdecay=grad_stepdecay
        self.t=0
        self.cost_history=[]

        self.theta_hist=[x_0]
        self.theta=x_0

    def update_params (self, gradient_reps=1):
        """This performs a single update of the model parameters"""
        self.t+=1

        c_k=self.grad_step()
        ### get the gradient
        ghat= self.gradient.evaluate(self.cost, self.theta, c_k, gradient_reps )

        ### determine the proposed step
        a_k=self.param_step()
        step=self.update.evaluate(ghat, a_k ,t)


        ### update the parameters
        new_theta=theta-step
        new_cost=self.cost.evaluate(new_theta)

        ### evaluate the objective function

        self.theta_hist.append(new_theta)
        self.theta=new_theta

        self.cost_history.append(new_cost)

    def fit(self, niter=10000, init_grad_reps=100):
        """This performs a set number of gradient descent descent iterations, along with some initialization"""
        pass
    def param_step(self):
        """This determines the step size used to update the model parameters.

        a_k= a/(t+A)**alpha"""
        return  (self.param_stepsize / (self.t+self.param_decay_offset)**self.param_stepdecay)

    def grad_step(self):
        """This determines the step size used to perturb the parameters during the gradient approximation"""
        return (self.grad_stepsize/(self.t)**self.grad_stepdecay)
